{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial Sentiment analysis with article headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:03:51.665742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import string\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "#the following two functions will allow to ignore padding of the input passed to the neural network\n",
    "from torch.nn.utils.rnn import pack_padded_sequence \n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>TeliaSonera TLSN said the offer is in line wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    neutral  \\\n",
       "0   neutral   \n",
       "1  negative   \n",
       "2  positive   \n",
       "3  positive   \n",
       "4  positive   \n",
       "5  positive   \n",
       "6  positive   \n",
       "7  positive   \n",
       "8  positive   \n",
       "9  positive   \n",
       "\n",
       "  According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .  \n",
       "0  Technopolis plans to develop in stages an area...                                                                               \n",
       "1  The international electronic industry company ...                                                                               \n",
       "2  With the new production plant the company woul...                                                                               \n",
       "3  According to the company 's updated strategy f...                                                                               \n",
       "4  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...                                                                               \n",
       "5  For the last quarter of 2010 , Componenta 's n...                                                                               \n",
       "6  In the third quarter of 2010 , net sales incre...                                                                               \n",
       "7  Operating profit rose to EUR 13.1 mn from EUR ...                                                                               \n",
       "8  Operating profit totalled EUR 21.1 mn , up fro...                                                                               \n",
       "9  TeliaSonera TLSN said the offer is in line wit...                                                                               "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('alldata.csv',delimiter=',',encoding='latin-1')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>TeliaSonera TLSN said the offer is in line wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0   neutral  Technopolis plans to develop in stages an area...\n",
       "1  negative  The international electronic industry company ...\n",
       "2  positive  With the new production plant the company woul...\n",
       "3  positive  According to the company 's updated strategy f...\n",
       "4  positive  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...\n",
       "5  positive  For the last quarter of 2010 , Componenta 's n...\n",
       "6  positive  In the third quarter of 2010 , net sales incre...\n",
       "7  positive  Operating profit rose to EUR 13.1 mn from EUR ...\n",
       "8  positive  Operating profit totalled EUR 21.1 mn , up fro...\n",
       "9  positive  TeliaSonera TLSN said the offer is in line wit..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['label','text']\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for duplicates\n",
    "data.text.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droping the duplicates\n",
    "data = data.drop_duplicates(subset = ['text'], keep = 'first')\n",
    "data.text.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    0\n",
       "text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the dataset\n",
    "data = data.sample(frac =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words is: 10291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## Extracting the vocabulary (list of words) of the dataset\n",
    "\n",
    "head_list = data['text'].tolist() # list of headlines\n",
    "\n",
    "\n",
    "# Stop words removal and lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk_tokenizer(headline):\n",
    "    #For the different steps of preprocessing, see this kaggle notebook https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "    #Removes stop words and handles lemmatization . Returns the list of tokens of the word\n",
    "    \n",
    "    #URLs removal\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    headline = url_pattern.sub(r'', headline)\n",
    "    \n",
    "    #HTML removal\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    headline= html_pattern.sub(r'', headline)\n",
    "    \n",
    "    #Punctuation removal\n",
    "    Punctuation = string.punctuation\n",
    "    headline = headline.translate(str.maketrans('', '', Punctuation))\n",
    "    \n",
    "    #Tokenization\n",
    "    head_tokens = word_tokenize(headline)    \n",
    "    \n",
    "    #stop words removal\n",
    "    head_tokens = [token for token in head_tokens if not (token.lower() in stop_words) ] \n",
    "    \n",
    "    #Lemmatization and conversion to lowercase\n",
    "    head_tokens = [lemmatizer.lemmatize(token.lower()) for token in head_tokens]\n",
    "    \n",
    "    return head_tokens\n",
    "\n",
    "head_list_token = [nltk_tokenizer(headline) for headline in head_list]\n",
    "words_list = head_list_token[0]\n",
    "for headline in head_list_token:\n",
    "    words_list = words_list+headline\n",
    "words_count = len(set(words_list)) #Number of unique words in the dataset\n",
    "print(\"The number of unique words is:\", words_count)\n",
    "\n",
    "lenlist = pd.Series([len(headline) for headline in head_list_token]) #lengths of the headlines in the dataset\n",
    "lenlist.plot(kind='box') #box plot for the headlines lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like just a couple of headlines have more than 30 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of reviews with more than 30 words is: 0.9716766590862104 %\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for headline in head_list_token:\n",
    "    if len(headline) > 30:\n",
    "        #print(\" \".join(review))\n",
    "        i += 1\n",
    "print(\"The proportion of reviews with more than 30 words is:\", (i/len(head_list_token))*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting the sentences to vectors, I'll set a maximum length since only few words have a length higher than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_list = [\" \".join(headline) for headline in head_list_token]\n",
    "\n",
    "# tokenizing (and preprocessing) the reviews\n",
    "max_words = int(words_count*0.8)# Number of most frequent words to use for the tokenization\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words = max_words,split=\" \") \n",
    "tokenizer.fit_on_texts(head_list) \n",
    "\n",
    "#vocab = tokenizer.word_index #vocabulary(a dictionnary containing each word and an integer that serves as index for the word )\n",
    "\n",
    "\n",
    "## Converting the dataset to an appropriate format for the neural network\n",
    "\n",
    "# Features\n",
    "indice_lists = tokenizer.texts_to_sequences(head_list) #each review is converted to a list of integer, where an integer is the indice of the coresponding word in the vocabulary\n",
    "indice_lists_pad = tf.keras.preprocessing.sequence.pad_sequences(indice_lists, maxlen=30,padding='pre',truncating='post', value=0.0) #add zeros for all the lists to have the same length. and truncate reviews that are too long.\n",
    "indice_lists_pad = torch.Tensor.long(torch.from_numpy(indice_lists_pad)) #converting the padded sequences to a tensor\n",
    "\n",
    "\n",
    "\n",
    "# Labels\n",
    "label_list = data['label'].tolist()\n",
    "label_dict = {\"positive\":0,\"negative\":1,\"neutral\":2}\n",
    "label_list = torch.tensor( [label_dict[element] for element in label_list])\n",
    "\n",
    "\n",
    "sample_size = label_list.size()[0]\n",
    "split = int(0.8*sample_size) #spliting index for training and testing set\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class training_data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.samples = [indice_lists_pad[:split,:],label_list[:split]]\n",
    "        #self.samples = self.samples.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples[1].size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[0][idx,:],self.samples[1][idx]\n",
    "\n",
    "class testing_data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.samples = [indice_lists_pad[split:,:],label_list[split:]]\n",
    "        #self.samples = self.samples.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples[1].size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[0][idx,:],self.samples[1][idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(training_data(), batch_size=200)\n",
    "test_dataloader = DataLoader(testing_data(), batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8233"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique indices\n",
    "max_words+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indice_list = indice_lists[0]\n",
    "#for headline in indice_lists:\n",
    "    #indice_list = indice_list+headline\n",
    "#indices_count = len(set(indice_list)) #Number of unique words in the dataset\n",
    "#print(\"The number of unique indicess is:\", indices_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) Model development (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myLSTM(\n",
      "  (embed): Embedding(8233, 20, padding_idx=0)\n",
      "  (lstm): LSTM(20, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self,batch_size,input_size,hidden_size): # Data is inputed in the neural network by minibatches.\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size #dimension of xi(vector representation of words)\n",
    "        self.hidden_size = hidden_size #dimension of states St\n",
    "        \n",
    "        # Word embedding layer \n",
    "        self.embed = nn.Embedding(8233, 20,padding_idx = 0)\n",
    "        \n",
    "        # RNN layer\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,batch_first = True)\n",
    "        \n",
    "        # Logistic regression layer\n",
    "        self.linear = nn.Linear(hidden_size,3) #this will be applied to each element of the batch. No need to specify the size of the batch\n",
    "        self.softmax = nn.Softmax(dim = 1) \n",
    "             #returns a vector of probabilities\n",
    "             #there are 3 categories: positive, negative, neutral (The probabilities for the 3 classes are returned)\n",
    "        \n",
    "    def forward(self, x,batch_size):\n",
    "       \n",
    "        \n",
    "        #print(batch_size)\n",
    "        h_0 = torch.zeros(1,batch_size,self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(1,batch_size,self.hidden_size).to(device)\n",
    "        \n",
    "        x = x.to(device)\n",
    "        e = self.embed(x)\n",
    "        \n",
    "        v = (x != 0).long()\n",
    "        v = torch.sum(v,dim =1)\n",
    "        featurelens = v.tolist()\n",
    "        \n",
    "        \n",
    "        e = e.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        r = self.lstm(e,(h_0,c_0))\n",
    "        \n",
    "        logits = self.softmax(self.linear(r[1][0]))\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "b_size = 200\n",
    "i_size = 20\n",
    "h_size = 50\n",
    "\n",
    "mylstm = myLSTM(batch_size = b_size,input_size = i_size,hidden_size = h_size).to(device)\n",
    "print(mylstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III) Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing loop setup\n",
    "\n",
    "# Training loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,printer = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        Y = torch.unsqueeze(y,1)\n",
    "        Y = Y.to(device)\n",
    "        pred = torch.squeeze(model(X,y.size()[0]),0)\n",
    "        #print(\"pred =\",pred.size())\n",
    "        \n",
    "        loss = loss_fn(pred,torch.squeeze(Y,1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            if printer:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Test loop\n",
    "def test_loop(dataloader, model, loss_fn,printer = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            Y = torch.unsqueeze(y,1)\n",
    "            Y = Y.to(device)\n",
    "            pred = torch.squeeze(model(X,y.size()[0]),0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            test_loss += loss_fn(pred,torch.squeeze(Y,1)).item()\n",
    "            \n",
    "            correct += (pred.argmax(1) == torch.squeeze(Y)).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    #print(\"correct =\",correct)\n",
    "    if printer:\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct #classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.098608  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.098608 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.098595  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 1.098598 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 1.098580  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.098586 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 1.098562  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 1.098572 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 1.098539  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 42.4%, Avg loss: 1.098553 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 1.098508  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 1.098526 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 1.098464  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 1.098486 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 1.098394  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 1.098424 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 1.098270  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.098324 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 1.098008  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 1.098134 \n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "loss: 1.097299  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.097540 \n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "loss: 1.095765  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.095750 \n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "loss: 1.093754  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.094833 \n",
      "\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "loss: 1.093706  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.094691 \n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "loss: 1.093053  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.094624 \n",
      "\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "loss: 1.092974  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.094529 \n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "loss: 1.092614  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.094215 \n",
      "\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "loss: 1.092268  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.094238 \n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "loss: 1.092405  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.093940 \n",
      "\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "loss: 1.092262  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.094409 \n",
      "\n",
      "Epoch 1001\n",
      "-------------------------------\n",
      "loss: 1.092376  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.094570 \n",
      "\n",
      "Epoch 1051\n",
      "-------------------------------\n",
      "loss: 1.092315  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.094181 \n",
      "\n",
      "Epoch 1101\n",
      "-------------------------------\n",
      "loss: 1.092226  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.094029 \n",
      "\n",
      "Epoch 1151\n",
      "-------------------------------\n",
      "loss: 1.092231  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.094192 \n",
      "\n",
      "Epoch 1201\n",
      "-------------------------------\n",
      "loss: 1.092205  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.093964 \n",
      "\n",
      "Epoch 1251\n",
      "-------------------------------\n",
      "loss: 1.092212  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.094137 \n",
      "\n",
      "Epoch 1301\n",
      "-------------------------------\n",
      "loss: 1.092224  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.094199 \n",
      "\n",
      "Epoch 1351\n",
      "-------------------------------\n",
      "loss: 1.092175  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.094304 \n",
      "\n",
      "Epoch 1401\n",
      "-------------------------------\n",
      "loss: 1.092341  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.093925 \n",
      "\n",
      "Epoch 1451\n",
      "-------------------------------\n",
      "loss: 1.092227  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.094095 \n",
      "\n",
      "Epoch 1501\n",
      "-------------------------------\n",
      "loss: 1.092253  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.094044 \n",
      "\n",
      "Epoch 1551\n",
      "-------------------------------\n",
      "loss: 1.092215  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.094059 \n",
      "\n",
      "Epoch 1601\n",
      "-------------------------------\n",
      "loss: 1.092326  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.093946 \n",
      "\n",
      "Epoch 1651\n",
      "-------------------------------\n",
      "loss: 1.092204  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.094071 \n",
      "\n",
      "Epoch 1701\n",
      "-------------------------------\n",
      "loss: 1.092225  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.094173 \n",
      "\n",
      "Epoch 1751\n",
      "-------------------------------\n",
      "loss: 1.092347  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.094077 \n",
      "\n",
      "Epoch 1801\n",
      "-------------------------------\n",
      "loss: 1.092221  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.094009 \n",
      "\n",
      "Epoch 1851\n",
      "-------------------------------\n",
      "loss: 1.092191  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.094014 \n",
      "\n",
      "Epoch 1901\n",
      "-------------------------------\n",
      "loss: 1.092194  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.094068 \n",
      "\n",
      "Epoch 1951\n",
      "-------------------------------\n",
      "loss: 1.092160  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.094086 \n",
      "\n",
      "Epoch 2001\n",
      "-------------------------------\n",
      "loss: 1.092424  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.094097 \n",
      "\n",
      "Epoch 2051\n",
      "-------------------------------\n",
      "loss: 1.092156  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.094131 \n",
      "\n",
      "Epoch 2101\n",
      "-------------------------------\n",
      "loss: 1.092221  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.094150 \n",
      "\n",
      "Epoch 2151\n",
      "-------------------------------\n",
      "loss: 1.092164  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.094264 \n",
      "\n",
      "Epoch 2201\n",
      "-------------------------------\n",
      "loss: 1.092213  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.094100 \n",
      "\n",
      "Epoch 2251\n",
      "-------------------------------\n",
      "loss: 1.092183  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.093961 \n",
      "\n",
      "Epoch 2301\n",
      "-------------------------------\n",
      "loss: 1.092181  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.094254 \n",
      "\n",
      "Epoch 2351\n",
      "-------------------------------\n",
      "loss: 1.092150  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.094228 \n",
      "\n",
      "Epoch 2401\n",
      "-------------------------------\n",
      "loss: 1.092230  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.094015 \n",
      "\n",
      "Epoch 2451\n",
      "-------------------------------\n",
      "loss: 1.092174  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.094193 \n",
      "\n",
      "Epoch 2501\n",
      "-------------------------------\n",
      "loss: 1.092181  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.093921 \n",
      "\n",
      "Epoch 2551\n",
      "-------------------------------\n",
      "loss: 1.092344  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.094133 \n",
      "\n",
      "Epoch 2601\n",
      "-------------------------------\n",
      "loss: 1.092152  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.093933 \n",
      "\n",
      "Epoch 2651\n",
      "-------------------------------\n",
      "loss: 1.092218  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.094141 \n",
      "\n",
      "Epoch 2701\n",
      "-------------------------------\n",
      "loss: 1.092287  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.094249 \n",
      "\n",
      "Epoch 2751\n",
      "-------------------------------\n",
      "loss: 1.092169  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.094068 \n",
      "\n",
      "Epoch 2801\n",
      "-------------------------------\n",
      "loss: 1.092153  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.093943 \n",
      "\n",
      "Epoch 2851\n",
      "-------------------------------\n",
      "loss: 1.092206  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.094194 \n",
      "\n",
      "Epoch 2901\n",
      "-------------------------------\n",
      "loss: 1.092288  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.094210 \n",
      "\n",
      "Epoch 2951\n",
      "-------------------------------\n",
      "loss: 1.092290  [    0/ 3869]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.094094 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n#plotting\\n#plt.plot(range(1,101),v_errors,label = \"Vanilla RNN\", color = \\'b\\')\\nplt.plot(range(1,101),l_errors,label = \"LSTM\", color = \\'r\\')\\nplt.xlabel(\"Number of iterations\")\\nplt.ylabel(\"classification error (in %)\")\\nplt.legend()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = 200 # batch size\n",
    "i_size = 20 # input size\n",
    "h_dim = 50\n",
    "\n",
    " \n",
    "########################################################################################\n",
    "## LSTM\n",
    "    \n",
    "mylstm = myLSTM(batch_size = b_size,input_size = i_size,hidden_size = h_size).to(device)\n",
    "    \n",
    "    \n",
    "## Training and testing\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1\n",
    "batch_size = 200\n",
    "epochs = 3000\n",
    "\n",
    "l_errors = [0]*epochs #will store classification errors\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.BCELoss()\n",
    "\n",
    "# specifying that the optimizer is stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(mylstm.parameters(), lr=learning_rate)\n",
    "#optimizer =torch.optim.Adam(mylstm.parameters(), lr=learning_rate)\n",
    "\n",
    "#Training and testing\n",
    "switch_lr1 = 0\n",
    "switch_lr2 = 0\n",
    "for t in range(epochs):\n",
    "    if t%50==0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, mylstm, loss_fn, optimizer,True)\n",
    "        l_errors[t] = test_loop(test_dataloader, mylstm, loss_fn,True)\n",
    "    else:\n",
    "        train_loop(train_dataloader, mylstm, loss_fn, optimizer)\n",
    "        l_errors[t] = test_loop(test_dataloader, mylstm, loss_fn)\n",
    "    if (l_errors[t] > 0.6) and (switch_lr1==0):\n",
    "        switch_lr1 = 1\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 0.1\n",
    "    if (l_errors[t] > 0.635) and (switch_lr2==0):\n",
    "        switch_lr2 = 1\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 0.001\n",
    "print(\"Done!\")\n",
    "    \n",
    "lstm_error = min(l_errors) #storing the best classification result on the test set\n",
    "    \n",
    "#Table.append([h_dim,vrnn_error,lstm_error])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#plotting\n",
    "#plt.plot(range(1,101),v_errors,label = \"Vanilla RNN\", color = 'b')\n",
    "plt.plot(range(1,101),l_errors,label = \"LSTM\", color = 'r')\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"classification error (in %)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
